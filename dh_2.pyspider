#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2017-08-03 17:58:11
# Project: zgjlxh2

from pyspider.libs.base_handler import *
from pyspider.database.mysql.mysqldb import MySql
import pyspider.libs.xxtools as tools
import urllib
from urllib import parse

class Handler(BaseHandler):
    def on_start(self):

        #1协会动态
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1005',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020101'} )
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1003002',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020102'} )
        #2行业资讯
        self.crawl('http://www.zgjlxh.org.cn/proscenium/forward?page=hehe1005',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020201'} )
        #3信息发布
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1006001',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020301'} )
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1006002',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020302'} )
        #4国际交流
        self.crawl('http://www.zgjlxh.org.cn/proscenium/forward?page=hehe1008',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020401'} )
        #5会展信息
        self.crawl('http://www.zgjlxh.org.cn/proscenium/forward?page=hehe1009',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020501'} )
        #6科技质量
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1010001',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020601'} )
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1010005',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020602'} )
        #7教育培训
        self.crawl('http://www.zgjlxh.org.cn/proscenium/forward?page=hehe1011',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020701'} )
        #8专题活动
        self.crawl('http://www.zgjlxh.org.cn/proscenium/forward?page=hehe1012',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020801'} )
        #9会员活动（会员资讯）
        self.crawl('http://www.zgjlxh.org.cn/proscenium/articlesList?page=hehe1014001',
                   fetch_type='js', callback=self.phantomjs_parser ,params={'crkind': '020901'} )

    @config(age=0)
    def phantomjs_parser(self, response):
        qs1 = parse.parse_qs(list(parse.urlparse(response.url))[4])#从上个页面获取参数crkind
         #翻页
        for each in response.doc('.pagination > a').items():
            url=each.attr.href
            print(url)
            #将参数crkind赋值给此页面
            bits = list(parse.urlparse(url))
            qs = parse.parse_qs(bits[4])
            qs['crkind']=qs1['crkind']
            bits[4] = parse.urlencode(qs, True)
            url = parse.urlunparse(bits)
            self.crawl(url, fetch_type='js', callback=self.phantomjs_parser) 
         #调用detail_page函数
        for each in response.doc('.newsUl a').items():
            url=each.attr.href
            #将参数crkind赋值给此页面
            bits = list(parse.urlparse(url))
            qs = parse.parse_qs(bits[4])
            qs['crkind']=qs1['crkind']
            bits[4] = parse.urlencode(qs, True)
            url = parse.urlunparse(bits)
            self.crawl(url, fetch_type='js', callback=self.detail_page) 
        
   
    @config(priority=2)
    def detail_page(self, response):
        site = "'http://www.zgjlxh.org.cn/"
        url = response.url
        title = response.doc('.newstitle').text()
        qs = parse.parse_qs(list(parse.urlparse(url))[4])     #从url参数字典分割出crkind的值,注意删掉'['和'''
        kind=str(qs['crkind']).strip("]").strip("[").strip("\'")   
        pagekey = tools.get_md5(title+url)
        content = response.doc('.fl > div').html()
        content_dom = tools.ext_cont(url, content)
        #获取cdate
        i=0
        cdate=''
        date=response.doc('.newsinfo > span').text()
        for i in range(0,10) :
            if ( date[i] == '-'):
                 continue
            cdate=cdate+date[i]
            i+=1
        
        return {
               "pagekey": pagekey,
               "site": site,
               "url": url,
               "title":title,
               "kind": kind,
               "content":content,
               "content_dom":content_dom,
               "cdate":cdate
              }    

    
    def on_result(self, result):
        print(result)
        if not result or not result['title']:
            return
        sql = MySql()
        sql.into('BUSI_CREDIT_INFO',**result)





