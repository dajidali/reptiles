#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2017-10-19 15:15:58
# Project: 86_salt

from pyspider.libs.base_handler import *
import pyspider.libs.xxtools as tools
from pyspider.database.mysql.mysqldb import MySql
import time

class Handler(BaseHandler):
    crawl_config = {
    }

    @every(minutes=24 * 60)
    def on_start(self):
        urls=[
            ['http://www.cnsalt.cn/list.asp?id=2','0860101'],#行业要闻
            ['http://www.cnsalt.cn/list.asp?id=3','0860102'],#协会活动
            ['http://www.cnsalt.cn/list.asp?id=6','0860201'],#海盐
            ['http://www.cnsalt.cn/list.asp?id=7','0860202'],#湖盐
            ['http://www.cnsalt.cn/list.asp?id=8','0860203'],#井矿盐
            ['http://www.cnsalt.cn/list.asp?id=9','0860204'],#气象信息
            ['http://www.cnsalt.cn/list.asp?id=35','0860301'],#运销工作
            ['http://www.cnsalt.cn/list.asp?id=36','0860302'],#运销管理
            ['http://www.cnsalt.cn/list.asp?id=37','0860303'],#企业经营
            ['http://www.cnsalt.cn/list.asp?id=11','0860401'],#简要分析
            ['http://www.cnsalt.cn/list.asp?id=12','0860402'],#化工资讯
            ['http://www.cnsalt.cn/list.asp?id=13','0860403'],#经济动态
            ['http://www.cnsalt.cn/list.asp?id=15','0860501'],#盐规盐法
            ['http://www.cnsalt.cn/list.asp?id=16','0860502'],#盐务管理
            ['http://www.cnsalt.cn/list.asp?id=18','0860503'],#盐政执法
            ['http://www.cnsalt.cn/list.asp?id=19','0860504'],#盐务研究
            ['http://www.cnsalt.cn/list.asp?id=26','0860601'],#科技动态
            ['http://www.cnsalt.cn/list.asp?id=27','0860602'],#质量与标准
            ['http://www.cnsalt.cn/list.asp?id=28','0860603'],#科技成果
            ['http://www.cnsalt.cn/list.asp?id=25','0860701'],#企业文化
        ]        
        for each in urls:
            self.crawl(each[0], callback=self.index_page, save=each[1])    

        #self.crawl('http://www.cnsalt.cn/list.asp?id=2', callback=self.index_page, save='0860101')

    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('#aa > a').items():
            self.crawl(each.attr.href, callback=self.index_page, save = response.save)
        for each in response.doc('td > li > a').items():
            self.crawl(each.attr.href, callback=self.detail_page, save = response.save)

    @config(priority=2)
    def detail_page(self, response):
        title = response.doc('.np').text()
        content = response.doc('#left > div').html()
        url = response.url
        site = 'http://www.cgcc.org.cn/'
        content_dom = tools.ext_cont(url, content)
        pagekey = tools.get_md5(title+url)
        date_string = response.doc('#fbxx span').eq(1).text()
        date = time.strptime(date_string[:date_string.find(' ')],'%Y/%m/%d')
        date = time.strftime('%Y%m%d',date)        
        kind = response.save
        return {
            "pagekey": pagekey,
            "url": url,
            "site": site,
            "title": title,
            "content_dom":content_dom,
            "cdate": date,
            "kind":kind
        }

    def on_result(self, result):
        print(result)
        if not result or not result['title']:
            return
        sql = MySql()
        sql.into('BUSI_CREDIT_INFO',**result)   
